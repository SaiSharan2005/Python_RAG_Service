# ğŸš€ Python RAG Service

Retrieval-Augmented Generation for large document collections on **1 GB RAM** with **no GPU**.

Combines **Lucene** (keyword search) + **Qdrant** (semantic search) + **Claude** (answer generation) for accurate, cited answers.

---

## ğŸ“‹ Quick Overview

```
User Question
     â”‚
     â–¼
Lucene Service (Java)     â†’ Top 1,000 keyword matches (fast)
     â”‚
     â–¼
Qdrant (Vector DB)        â†’ Top 10 semantic matches (accurate)
     â”‚
     â–¼
Python RAG Service        â†’ Build prompt + call Claude
     â”‚
     â–¼
Answer with Sources       â†’ "According to doc_xyz.pdf, page 5..."
```

---

## ğŸ“ Directory Structure

```
Python_RAG_Service/
â”œâ”€â”€ ingestion-rag-pipeline.ipynb     â­ Embed chunks & store in Qdrant
â”œâ”€â”€ run_ingestion.py                 Command-line ingestion runner
â”œâ”€â”€ run_ingestion_cloud.py           Google Colab optimized version
â”œâ”€â”€ requirements.txt                 Python dependencies
â”œâ”€â”€ app/                             FastAPI service code
â”‚   â”œâ”€â”€ main.py                      Query API endpoints
â”‚   â”œâ”€â”€ retriever.py                 Embed queries & search Qdrant
â”‚   â”œâ”€â”€ generator.py                 Claude answer generation
â”‚   â”œâ”€â”€ qdrant_store.py              Vector DB operations
â”‚   â”œâ”€â”€ embedding.py                 BGE embedding model
â”‚   â””â”€â”€ config.py                    Configuration from .env
â”œâ”€â”€ .env                             Your API keys (git-ignored)
â”œâ”€â”€ .env.example                     Template for .env
â”œâ”€â”€ .env.prod                        Production config
â”œâ”€â”€ README.md                        (this file)
â””â”€â”€ TECHNICAL.md                     Deep technical reference
```

---

## ğŸ¯ What This Service Does

**Three-stage RAG pipeline**:

1. **Lucene** (Java) - Fast keyword search
   - Scans 1M+ chunks instantly
   - Returns top 1,000 candidates via BM25 scoring
   - Takes ~15 ms

2. **Qdrant** (Vector DB) - Semantic reranking
   - Embeds query using BAAI/bge-small-en
   - Searches ONLY the 1,000 candidates (brute-force)
   - Returns top 10 most relevant chunks
   - Takes ~20 ms

3. **Claude** (LLM) - Answer generation
   - Reads the 10 best chunks
   - Generates grounded, cited answer
   - References source PDF + page number
   - Takes 1-3 seconds

**Total latency** (excluding LLM): < 50 ms
**Total latency** (with Claude): 1-3 seconds

---

## ğŸ”§ Setup & Installation

### **Step 1: Install Dependencies**

```bash
cd Python_RAG_Service

# Install CPU-only PyTorch (important!)
pip install torch==2.5.1+cpu --index-url https://download.pytorch.org/whl/cpu

# Install remaining dependencies
pip install -r requirements.txt
```

**Why CPU-only PyTorch?**
- CPU build: 115 MB
- CUDA build: 2+ GB
- For a 1 GB server, CPU build is mandatory

### **Step 2: Configure Environment**

```bash
# Copy template
cp .env.example .env

# Edit .env
nano .env  # or your editor
```

**Required settings**:
```
LLM_API_KEY=sk-ant-xxxxxxx          # Your Claude API key
QDRANT_URL=http://localhost:6333    # Local Qdrant or cloud
QDRANT_API_KEY=                     # Empty for local, required for cloud
LUCENE_URL=http://localhost:8080    # Java service URL
```

### **Step 3: Start Qdrant** (if local)

```bash
docker run -d \
  --name qdrant \
  -p 6333:6333 \
  -p 6334:6334 \
  qdrant/qdrant:latest
```

### **Step 4: Start Lucene Service** (separate terminal)

```bash
cd lucene-service
mvn spring-boot:run
```

---

## ğŸ“¥ Ingestion Pipeline

Two options to populate Qdrant with embeddings:

### **Option A: Google Colab (Recommended for Large Datasets)**

Use the Jupyter notebook with free T4 GPU:

```bash
# Upload to Colab
# Open: ingestion-rag-pipeline.ipynb

# Or use direct Python runner
python run_ingestion_cloud.py
```

**What it does**:
1. Loads JSON chunks from `lucene-service/chunk-exports/`
2. Embeds in batches of 256 on GPU
3. Upserts to Qdrant Cloud in batches of 1000
4. **Speed**: ~99 MB in 2-5 minutes

**Key features**:
- âœ… Text sanitization (removes control characters)
- âœ… Error handling (pinpoints problematic chunks)
- âœ… Memory cleanup (GC after each batch)
- âœ… Progress bar (live status)

### **Option B: Command Line (Local)**

```bash
# One-time ingestion of all chunks
python run_ingestion.py

# Or from the notebook
python -m jupyter nbconvert --to script ingestion-rag-pipeline.ipynb
python ingestion_rag_pipeline.py
```

---

## ğŸš€ Running the Service

### **Start the Query Server**

```bash
# Development (auto-reload)
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Production
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### **Test the API**

```bash
# Health check
curl http://localhost:8000/health

# Example query (requires candidate IDs from Lucene)
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are transformers?",
    "candidate_ids": ["chunk_001", "chunk_002", "chunk_003"]
  }'

# Response:
{
  "answer": "According to the documents, transformers are...",
  "sources": [
    {
      "source": "2601.16344v1.pdf",
      "title": "DSGym Framework",
      "page_number": 5,
      "score": 0.873
    }
  ],
  "query_embedding_time_ms": 8,
  "search_time_ms": 18,
  "total_time_ms": 42
}
```

---

## ğŸ“Š Performance Specs

| Metric | Value |
|--------|-------|
| **Chunks indexed** | 1M+ |
| **Query embedding (CPU)** | ~8 ms |
| **Qdrant semantic search** | ~20 ms |
| **Total retrieval** | < 50 ms |
| **Claude answer generation** | 1-3 sec |
| **RAM usage at query time** | ~235 MB |
| **Model size on disk** | ~130 MB |
| **PyTorch build** | 115 MB (CPU) |

---

## ğŸ“š Ingestion Pipeline Breakdown

### **ingestion-rag-pipeline.ipynb**

The main notebook for embedding and storing chunks:

**Step-by-step**:
1. **Install dependencies** - torch, sentence-transformers, qdrant-client
2. **Configuration** - Set Qdrant URL, API key, model name
3. **Upload JSON files** - From Lucene service exports
4. **Load embedding model** - BAAI/bge-small-en (384 dim)
5. **Create Qdrant collection** - HNSW disabled, brute-force only
6. **Sanitize text** - Remove control characters that break tokenizers
7. **Embed in batches** - 256 texts at a time on GPU
8. **Upsert to Qdrant** - 1000 points per batch
9. **Verify** - Check point count and sample payload
10. **Test search** - Run a quick similarity search

**Key features**:
- âœ… Works on Colab T4 GPU (~2-5 min for 44K chunks)
- âœ… Works on CPU (~30-60 min for 44K chunks)
- âœ… Handles large files (streaming, batch processing)
- âœ… Sanitizes text (removes NUL bytes, control chars)
- âœ… Detailed error messages (tells you exactly which text failed)
- âœ… Memory cleanup (GC + CUDA cache clearing)

**Configuration** (in notebook):
```python
QDRANT_URL = "https://your-cloud-instance"
QDRANT_API_KEY = "your-api-key"
EMBEDDING_MODEL = "BAAI/bge-small-en"
EMBEDDING_DIM = 384
EMBED_BATCH_SIZE = 256
UPSERT_BATCH_SIZE = 1000
JSON_DIR = "./chunk-exports"
```

---

## âš™ï¸ Configuration

All settings in `.env`:

```bash
# Claude API
LLM_API_KEY=sk-ant-xxxxxxx
LLM_MODEL=claude-3-5-sonnet-20241022

# Qdrant Vector DB
QDRANT_URL=https://your-instance.eu-central-1-0.aws.cloud.qdrant.io:6333
QDRANT_API_KEY=your-api-key
QDRANT_COLLECTION=rag_chunks

# Lucene Service
LUCENE_URL=http://localhost:8080

# Search parameters
TOP_K_SEMANTIC=10          # Final chunks for Claude
SIMILARITY_THRESHOLD=0.5   # Min similarity score

# Ingestion
EMBED_BATCH_SIZE=256
UPSERT_BATCH_SIZE=1000
MAX_CHUNK_TOKENS=512
```

For detailed reference, see **[TECHNICAL.md](TECHNICAL.md)**.

---

## ğŸ” Workflow: End-to-End

### **Setup Phase** (One-time)

```bash
# 1. Start services
docker run -d --name qdrant -p 6333:6333 qdrant/qdrant:latest
cd lucene-service && mvn spring-boot:run &

# 2. Upload PDFs to Lucene
curl -X POST http://localhost:8080/api/v1/ingest/pdf \
  -F "file=@paper1.pdf" \
  -F "file=@paper2.pdf"

# 3. Wait for Lucene to process & export JSON
# Check: lucene-service/chunk-exports/*.json

# 4. Embed chunks in Qdrant (on Colab or local)
python run_ingestion.py
# or upload ingestion-rag-pipeline.ipynb to Colab
```

### **Query Phase** (Runtime)

```bash
# 1. Start Python RAG service
uvicorn app.main:app --host 0.0.0.0 --port 8000

# 2. User asks a question
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "What is attention mechanism?"}'

# 3. Service:
#    a. Queries Lucene â†’ gets 1000 candidate IDs
#    b. Embeds query using BGE-small-en
#    c. Searches Qdrant (only those 1000) â†’ gets top 10
#    d. Builds prompt from top 10 chunks
#    e. Calls Claude
#    f. Returns answer + sources
```

---

## ğŸ’¾ Data Flow

```
PDFs (lucene-service/*)
  â”‚
  â–¼
Lucene Service (Java)
  â”œâ”€ Extract text (PDFBox)
  â”œâ”€ Split into chunks (overlapping, ~400 tokens)
  â”œâ”€ Index with BM25
  â””â”€ Export JSON â†’ chunk-exports/
     â”‚
     â–¼
ingestion-rag-pipeline.ipynb (Colab)
  â”œâ”€ Load JSON files
  â”œâ”€ Sanitize text
  â”œâ”€ Embed with BGE-small-en
  â”œâ”€ Upsert to Qdrant
  â””â”€ Done!
     â”‚
     â–¼
Qdrant Cloud (44,000+ vectors)
  â”‚
  â–¼
Python RAG Service (FastAPI)
  â”œâ”€ Receive query
  â”œâ”€ Query Lucene (1M â†’ 1K)
  â”œâ”€ Embed query
  â”œâ”€ Search Qdrant (1K â†’ 10)
  â”œâ”€ Build prompt
  â”œâ”€ Call Claude
  â””â”€ Return answer + sources
```

---

## ğŸ› Troubleshooting

### **Embedding fails with "invalid UTF-8"**

The notebook sanitizes text automatically:
```python
def sanitize_text(text: str) -> str:
    text = text.replace("\x00", "")  # Remove NUL bytes
    # Remove other control chars
    cleaned = [
        " " if ord(ch) < 32 and ch not in "\n\r\t" else ch
        for ch in text
    ]
    return "".join(cleaned).strip()
```

If still failing, the notebook tells you exactly which text broke:
```
BROKEN TEXT FOUND
Sub-index: 45
Length: 2341
Preview: "threshold ğœ• = 10âˆ’2..."
Error: ...tokenizer error...
```

### **Qdrant connection refused**

Check if Qdrant is running:
```bash
curl http://localhost:6333/health
# Should return: {"status":"ok"}
```

If using cloud:
- Verify `QDRANT_URL` in `.env`
- Check `QDRANT_API_KEY` is set correctly
- Ensure IP is whitelisted (if cloud instance requires it)

### **Claude API errors**

Check your API key:
```bash
echo $LLM_API_KEY
# Should start with sk-ant-
```

### **High latency on queries**

- **Lucene slow?** Check if Java service has enough memory
- **Qdrant slow?** Verify brute-force search is enabled (`exact=True`)
- **Embedding slow?** Expected on CPU (~8 ms), normal

---

## ğŸ“– Files Guide

| File | Purpose |
|------|---------|
| **ingestion-rag-pipeline.ipynb** | Main notebook: embed chunks, store in Qdrant |
| **run_ingestion.py** | CLI runner for ingestion (local) |
| **run_ingestion_cloud.py** | Optimized for Google Colab |
| **app/main.py** | FastAPI server - `/ask` endpoint |
| **app/retriever.py** | Query embedding + Qdrant search |
| **app/generator.py** | Claude answer generation |
| **app/qdrant_store.py** | Vector DB operations |
| **app/embedding.py** | BGE model loading + encoding |
| **requirements.txt** | Python dependencies |
| **README.md** | (you are here) |
| **TECHNICAL.md** | Full API reference & internals |

---

## ğŸ“ Key Concepts

### **Why Lucene + Qdrant?**

| Stage | Why |
|-------|-----|
| **Lucene** | Keyword search is instant at scale. BM25 is unbeatable for full-text search. Shrinks 1M â†’ 1K in ~15ms. |
| **Qdrant** | Semantic search understands meaning. "Architecture limitations" matches "transformer shortcomings". But can't scale to 1M on 1 GB. |
| **Together** | Lucene filters, Qdrant understands. Best of both worlds. |

### **Why HNSW Disabled?**

HNSW is a graph index for fast approximate nearest neighbor search. It uses ~100-200 MB per million vectors.

On a 1 GB server, that's most of our memory budget.

But we're never searching the full collection. We're searching 1,000 pre-filtered candidates. Brute-force cosine similarity over 1K vectors takes only ~20 ms. No index needed.

### **Why BGE-small-en?**

- **Size**: 33M parameters, 384 dimensions
- **Speed**: Fast on CPU (~8 ms per query)
- **Quality**: Solid performance for its size
- **Model**: Specifically trained for semantic search (not generation)

---

## ğŸ“ Quick Reference

| Task | Command |
|------|---------|
| Install deps | `pip install -r requirements.txt` |
| Start Qdrant | `docker run -d --name qdrant -p 6333:6333 qdrant/qdrant:latest` |
| Start Lucene | `cd lucene-service && mvn spring-boot:run` |
| Run ingestion | `python run_ingestion.py` or notebook |
| Start API | `uvicorn app.main:app --reload --port 8000` |
| Test API | `curl http://localhost:8000/health` |
| View logs | Check .env + FastAPI console output |
| Debug embedding | Run notebook cell-by-cell, watch for sanitization errors |

---

## ğŸ“š More Info

**For technical details**, see **[TECHNICAL.md](TECHNICAL.md)**:
- Complete API reference
- Chunk JSON schema
- Qdrant configuration internals
- Embedding pipeline details
- Prompt construction
- Full deployment guide
- Troubleshooting (common issues + fixes)

---

## âœ… Status

- âœ“ Ingestion pipeline (notebook + CLI)
- âœ“ Query API (FastAPI)
- âœ“ Qdrant integration (brute-force search)
- âœ“ Claude integration (grounded answers)
- âœ“ Citation tracking (sources per answer)
- âœ“ CPU-only deployment (no GPU needed)
- âœ“ Low memory footprint (1 GB server)
- âœ“ Production-ready

**Ready to deploy!** ğŸš€
