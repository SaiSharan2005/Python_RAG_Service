{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fadfd32e",
   "metadata": {
    "papermill": {
     "duration": 0.003946,
     "end_time": "2026-02-18T17:48:26.106113",
     "exception": false,
     "start_time": "2026-02-18T17:48:26.102167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Production RAG — Ingestion Pipeline\n",
    "\n",
    "This notebook embeds chunked documents and stores them in Qdrant.  \n",
    "Designed to run on **Google Colab** (free GPU) so ingestion finishes in minutes, not hours.\n",
    "\n",
    "### What this notebook does\n",
    "1. Installs dependencies (torch GPU, sentence-transformers, qdrant-client)\n",
    "2. Uploads the JSON chunk exports from the Lucene service\n",
    "3. Loads `BAAI/bge-small-en` embedding model on GPU\n",
    "4. Creates a Qdrant collection (HNSW disabled, brute-force only)\n",
    "5. Streams each JSON file → embeds in batches → upserts to Qdrant\n",
    "6. Verifies the final point count\n",
    "\n",
    "### Requirements\n",
    "- The 11 JSON files from `lucene-service/chunk-exports/`\n",
    "- A running Qdrant instance (local, Docker, or Qdrant Cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be820e52",
   "metadata": {
    "papermill": {
     "duration": 0.002936,
     "end_time": "2026-02-18T17:48:26.112149",
     "exception": false,
     "start_time": "2026-02-18T17:48:26.109213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84a8c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:28:16.900200Z",
     "iopub.status.busy": "2026-02-16T11:28:16.899584Z",
     "iopub.status.idle": "2026-02-16T11:29:05.162904Z",
     "shell.execute_reply": "2026-02-16T11:29:05.162031Z",
     "shell.execute_reply.started": "2026-02-16T11:28:16.900175Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2026-02-18T17:48:26.115024",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers==3.3.1 qdrant-client==1.12.1  torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9a0a4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "All settings in one place. Update `QDRANT_HOST` / `QDRANT_API_KEY` to point to your Qdrant instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8fa637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:33:19.037559Z",
     "iopub.status.busy": "2026-02-16T11:33:19.037198Z",
     "iopub.status.idle": "2026-02-16T11:33:19.045004Z",
     "shell.execute_reply": "2026-02-16T11:33:19.044101Z",
     "shell.execute_reply.started": "2026-02-16T11:33:19.037531Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Generator\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance,\n",
    "    HnswConfigDiff,\n",
    "    OptimizersConfigDiff,\n",
    "    PointStruct,\n",
    "    VectorParams,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# ── Qdrant Cloud ────────────────────────────────────────────\n",
    "QDRANT_URL = \"https://b210317b-feb7-4514-89c0-44668fffeba0.eu-central-1-0.aws.cloud.qdrant.io:6333\"\n",
    "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.MC3P9BZdG63yqfXKnG3udz5XAyS-wbqctc52fEcmYGk\"\n",
    "COLLECTION_NAME = \"rag_chunks\"\n",
    "\n",
    "# ── Embedding ───────────────────────────────────────────────\n",
    "EMBEDDING_MODEL = \"BAAI/bge-small-en\"\n",
    "EMBEDDING_DIM = 384\n",
    "EMBED_BATCH_SIZE = 256             # GPU can handle larger batches\n",
    "\n",
    "# ── Ingestion ───────────────────────────────────────────────\n",
    "UPSERT_BATCH_SIZE = 1000           # Points per Qdrant upsert call\n",
    "JSON_DIR = \"./chunk-exports\"       # Upload your JSON files here\n",
    "\n",
    "# ── Device ──────────────────────────────────────────────────\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device : {DEVICE}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU    : {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29141d15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 3. Upload JSON Chunk Exports\n",
    "\n",
    "Upload the 11 JSON files exported by the Lucene service.  \n",
    "Two options:\n",
    "- **Option A**: Google Colab file upload (small files)\n",
    "- **Option B**: Mount Google Drive (recommended for ~99 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f62af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:03:38.237651Z",
     "iopub.status.busy": "2026-02-16T12:03:38.237307Z",
     "iopub.status.idle": "2026-02-16T12:03:38.249406Z",
     "shell.execute_reply": "2026-02-16T12:03:38.248489Z",
     "shell.execute_reply.started": "2026-02-16T12:03:38.237619Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ── Option A: Upload directly (works for small files) ──────\n",
    "# Uncomment the lines below to upload via Colab UI:\n",
    "\n",
    "# from google.colab import files\n",
    "# os.makedirs(JSON_DIR, exist_ok=True)\n",
    "# uploaded = files.upload()\n",
    "# for name, data in uploaded.items():\n",
    "#     with open(os.path.join(JSON_DIR, name), 'wb') as f:\n",
    "#         f.write(data)\n",
    "# print(f\"Uploaded {len(uploaded)} file(s)\")\n",
    "\n",
    "\n",
    "# ── Option B: Mount Google Drive (recommended) ─────────────\n",
    "# Upload the JSON files to your Drive first, then:\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "JSON_DIR = \"/kaggle/input/lucenexrag/\"  # adjust path\n",
    "\n",
    "\n",
    "# ── Verify files ───────────────────────────────────────────\n",
    "json_files = sorted(Path(JSON_DIR).glob(\"*.json\"))\n",
    "total_size = sum(f.stat().st_size for f in json_files) / (1024 * 1024)\n",
    "print(f\"Found {len(json_files)} JSON file(s) — {total_size:.1f} MB total\")\n",
    "for f in json_files:\n",
    "    print(f\"  {f.name} ({f.stat().st_size / (1024*1024):.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac232c72",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 4. Load Embedding Model\n",
    "\n",
    "Loads `BAAI/bge-small-en` (33M params, 384 dimensions).  \n",
    "On Colab GPU this takes ~5 seconds. On CPU it takes ~30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35f9f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:03:38.586159Z",
     "iopub.status.busy": "2026-02-16T12:03:38.585874Z",
     "iopub.status.idle": "2026-02-16T12:03:39.772209Z",
     "shell.execute_reply": "2026-02-16T12:03:39.771465Z",
     "shell.execute_reply.started": "2026-02-16T12:03:38.586131Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_embedding_model(model_name: str, device: str) -> SentenceTransformer:\n",
    "    \"\"\"Load and return the sentence-transformer model.\n",
    "\n",
    "    The model is put in eval mode and moved to the specified device.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {model_name} on {device}...\")\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded. Max sequence length: {model.max_seq_length}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_embedding_model(EMBEDDING_MODEL, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf4d48",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 5. Connect to Qdrant & Create Collection\n",
    "\n",
    "Creates the collection with **HNSW fully disabled** (`m=0`).  \n",
    "This is intentional — the deployed server has only 1 GB RAM.  \n",
    "Search uses `exact=True` (brute-force) over ~1000 Lucene-filtered candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da793962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:03:39.773655Z",
     "iopub.status.busy": "2026-02-16T12:03:39.773418Z",
     "iopub.status.idle": "2026-02-16T12:03:40.417374Z",
     "shell.execute_reply": "2026-02-16T12:03:40.416809Z",
     "shell.execute_reply.started": "2026-02-16T12:03:39.773632Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def connect_qdrant(url: str, api_key: str) -> QdrantClient:\n",
    "    \"\"\"Connect to Qdrant Cloud.\n",
    " \n",
    "    Uses the full URL + API key for authentication.\n",
    "    \"\"\"\n",
    "    client = QdrantClient(url=url, api_key=api_key, timeout=120)\n",
    "    collections = [c.name for c in client.get_collections().collections]\n",
    "    print(f\"Connected to Qdrant Cloud. Existing collections: {collections}\")\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_collection(\n",
    "    client: QdrantClient,\n",
    "    name: str,\n",
    "    dim: int,\n",
    ") -> None:\n",
    "    \"\"\"Create the vector collection with HNSW disabled.\n",
    "\n",
    "    Design choices:\n",
    "      - HnswConfigDiff(m=0)       -> no HNSW graph, saves RAM\n",
    "      - indexing_threshold=0       -> no automatic index building\n",
    "      - on_disk_payload=True       -> payloads stored on disk, not RAM\n",
    "      - Distance.COSINE            -> cosine similarity scoring\n",
    "    \"\"\"\n",
    "    collections = [c.name for c in client.get_collections().collections]\n",
    "    if name in collections:\n",
    "        info = client.get_collection(name)\n",
    "        print(f\"Collection '{name}' already exists with {info.points_count} points.\")\n",
    "        return\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=dim,\n",
    "            distance=Distance.COSINE,\n",
    "        ),\n",
    "        hnsw_config=HnswConfigDiff(m=0),\n",
    "        optimizers_config=OptimizersConfigDiff(\n",
    "            indexing_threshold=0,\n",
    "        ),\n",
    "        on_disk_payload=True,\n",
    "    )\n",
    "    print(f\"Created collection '{name}' (HNSW disabled, on-disk payload).\")\n",
    "\n",
    "\n",
    "qdrant = connect_qdrant(QDRANT_URL, QDRANT_API_KEY)\n",
    "create_collection(qdrant, COLLECTION_NAME, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3c312",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": "---\n## 6. Helper Functions\n\nCore utilities used by the ingestion loop:  \n- **`to_qdrant_id`** — deterministic UUID conversion (Qdrant needs valid UUIDs)  \n- **`load_json_records`** — loads and validates records from a JSON file  \n- **`build_payload`** — extracts ALL metadata fields for storage (9 fields total):\n  - chunk_id, content (document identification)\n  - source, title, author (document metadata)\n  - page_number, chunk_index, chunk_position (positional metadata)\n  - token_count (for context window management)\n  - document_id (for tracking)\n- **`embed_batch`** — embeds a list of texts using the model\n\n**Note:** The `build_payload` function now matches the production RAG service (`app/ingestion.py`) to ensure consistency across Colab and production environments."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b738c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:03:40.418578Z",
     "iopub.status.busy": "2026-02-16T12:03:40.418290Z",
     "iopub.status.idle": "2026-02-16T12:03:40.429071Z",
     "shell.execute_reply": "2026-02-16T12:03:40.428292Z",
     "shell.execute_reply.started": "2026-02-16T12:03:40.418546Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": "def to_qdrant_id(chunk_id: str) -> str:\n    \"\"\"Convert an arbitrary chunk ID string to a valid Qdrant UUID.\n\n    Qdrant accepts only unsigned integers or valid UUIDs as point IDs.\n    Lucene exports IDs like '5c4a9c97-..._p1_c0_088c5634' which are not\n    valid UUIDs. uuid5 produces a deterministic UUID from any string,\n    so the same chunk_id always maps to the same Qdrant point ID.\n\n    IMPORTANT: This same function is used in the deployed rag-service\n    (qdrant_store.py) so the IDs match at query time.\n    \"\"\"\n    return str(uuid.uuid5(uuid.NAMESPACE_URL, chunk_id))\n\n\ndef load_json_records(path: Path) -> List[Dict[str, Any]]:\n    \"\"\"Load a JSON file and return validated records.\n\n    Filters out records with missing ID or non-string content.\n    On Colab we have plenty of RAM so json.load() is fine.\n    \"\"\"\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    records = []\n    skipped = 0\n    for rec in data:\n        rid = rec.get(\"id\")\n        content = rec.get(\"content\")\n        if rid is None or not isinstance(content, str) or not content.strip():\n            skipped += 1\n            continue\n        records.append(rec)\n\n    print(f\"  Loaded {len(records)} valid records, skipped {skipped}\")\n    return records\n\n\n\nimport unicodedata\n\ndef sanitize_text(text: str) -> str:\n    \"\"\"Clean text for the tokenizer.\n\n    Removes NUL bytes and other control characters that can\n    cause the Rust tokenizer to reject the input.\n    \"\"\"\n    # Remove NUL bytes\n    text = text.replace(\"\\x00\", \"\")\n    # Replace other control chars (keep newline, tab, carriage return)\n    cleaned = []\n    for ch in text:\n        if ord(ch) < 32 and ch not in \"\\n\\r\\t\":\n            cleaned.append(\" \")\n        else:\n            cleaned.append(ch)\n    return \"\".join(cleaned).strip()\n\n\n\n\ndef build_payload(record: Dict[str, Any], clean_content: str) -> Dict[str, Any]:\n    \"\"\"Extract metadata payload to store alongside the vector.\n\n    Stores all essential metadata fields for response construction and context:\n    - Document identification (chunk_id, document_id)\n    - Content (content)\n    - Document metadata (source, title, author)\n    - Positional metadata (page_number, chunk_index, chunk_position)\n    - Statistics (token_count)\n\n    This matches the production RAG service payload structure in app/ingestion.py\n    \"\"\"\n    metadata = record.get(\"metadata\", {})\n    return {\n        \"chunk_id\": record.get(\"id\", \"\"),\n        \"content\": clean_content,\n        \"source\": metadata.get(\"source\", \"\"),\n        \"title\": metadata.get(\"title\", \"\"),\n        \"author\": metadata.get(\"author\", \"\"),\n        \"page_number\": metadata.get(\"page_number\"),\n        \"chunk_index\": metadata.get(\"chunk_index\"),\n        \"chunk_position\": metadata.get(\"chunk_position\"),\n        \"token_count\": metadata.get(\"token_count\"),\n        \"document_id\": record.get(\"document_id\", \"\"),\n    }\n\ndef embed_batch(\n    model: SentenceTransformer,\n    texts: List[str],\n    batch_size: int = 256,\n) -> List[List[float]]:\n    \"\"\"Embed a list of texts and return as list of float vectors.\n\n    Uses torch.no_grad() to save memory. Normalizes embeddings\n    for cosine similarity. Cleans up GPU memory after encoding.\n    \"\"\"\n    with torch.no_grad():\n        embeddings = model.encode(\n            texts,\n            batch_size=batch_size,\n            show_progress_bar=False,\n            normalize_embeddings=True,\n            convert_to_numpy=True,\n        )\n    result = embeddings.tolist()\n    del embeddings\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    return result\n\n\nprint(\"Helper functions loaded.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ad150",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:03:40.430942Z",
     "iopub.status.busy": "2026-02-16T12:03:40.430714Z",
     "iopub.status.idle": "2026-02-16T12:03:40.448610Z",
     "shell.execute_reply": "2026-02-16T12:03:40.448027Z",
     "shell.execute_reply.started": "2026-02-16T12:03:40.430919Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # math_equation = ['often suffer from \"bleeding\" artifacts, where weights spill onto unconnected mesh parts (see UniRig/Puppeteer columns). TokenRig (Ours) produces clean, locally coherent influence maps that closely mat']\n",
    "# # embed_batch(model, math_equation)\n",
    "# bad = sanitize_text(\"threshold \\uD835\\uDF00 = 10−2\")\n",
    "# embed_batch(model, [bad])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60de3e9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 7. Run Ingestion Pipeline\n",
    "\n",
    "This is the main loop:  \n",
    "1. Loads each JSON file with `json.load()`  \n",
    "2. Sanitizes text content for the tokenizer  \n",
    "3. Embeds in batches of 256 on GPU  \n",
    "4. Upserts to Qdrant Cloud in batches of 1000  \n",
    "5. Reports progress with a live progress bar  \n",
    "\n",
    "On a **Colab T4 GPU**, expect ~99 MB of chunks to finish in **2-5 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bc532",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd78d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:03:40.450162Z",
     "iopub.status.busy": "2026-02-16T12:03:40.449539Z",
     "iopub.status.idle": "2026-02-16T12:06:07.802056Z",
     "shell.execute_reply": "2026-02-16T12:06:07.801117Z",
     "shell.execute_reply.started": "2026-02-16T12:03:40.450121Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _process_and_upsert(\n",
    "    records: List[Dict[str, Any]],\n",
    "    model: SentenceTransformer,\n",
    "    client: QdrantClient,\n",
    "    collection: str,\n",
    "    embed_batch_size: int,\n",
    ") -> int:\n",
    "    \"\"\"Embed a batch of records and upsert to Qdrant.\"\"\"\n",
    "    clean_texts = [sanitize_text(r[\"content\"]) for r in records]\n",
    "    texts = clean_texts\n",
    "    \n",
    "    raw_ids = [r[\"id\"] for r in records]\n",
    "    ids = [to_qdrant_id(rid) for rid in raw_ids]\n",
    "    \n",
    "    payloads = [\n",
    "        build_payload(records[i], clean_texts[i])\n",
    "        for i in range(len(records))\n",
    "    ]\n",
    "\n",
    "    # Embed in sub-batches\n",
    "    all_vectors: List[List[float]] = []\n",
    "\n",
    "    for i in range(0, len(texts), embed_batch_size):\n",
    "        sub = texts[i : i + embed_batch_size]\n",
    "    \n",
    "        try:\n",
    "            vecs = embed_batch(model, sub, batch_size=embed_batch_size)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"\\nEmbedding failed!\")\n",
    "            print(\"Batch start index:\", i)\n",
    "            print(\"Sub-batch length:\", len(sub))\n",
    "            print(\"Error:\", repr(e))\n",
    "        \n",
    "            print(\"\\nTesting each item individually...\")\n",
    "        \n",
    "            for j, t in enumerate(sub):\n",
    "                try:\n",
    "                    model.encode([t])\n",
    "                except Exception as inner:\n",
    "                    print(\"\\nBROKEN TEXT FOUND\")\n",
    "                    print(\"Sub-index:\", j)\n",
    "                    print(\"Length:\", len(t))\n",
    "                    print(\"Preview:\", repr(t[:200]))\n",
    "                    print(\"Error:\", repr(inner))\n",
    "                    raise\n",
    "        \n",
    "            raise\n",
    "    \n",
    "        all_vectors.extend(vecs)    \n",
    "        del vecs\n",
    "\n",
    "    # Build points and upsert\n",
    "    points = [\n",
    "        PointStruct(id=ids[j], vector=all_vectors[j], payload=payloads[j])\n",
    "        for j in range(len(ids))\n",
    "    ]\n",
    "    client.upsert(collection_name=collection, points=points, wait=True)\n",
    "\n",
    "    del all_vectors, points\n",
    "    gc.collect()\n",
    "    return len(ids)\n",
    "\n",
    "\n",
    "# ── Main ingestion loop ────────────────────────────────────\n",
    "total_ingested = 0\n",
    "total_skipped = 0\n",
    "start_time = time.time()\n",
    "pbar = tqdm(desc=\"Ingesting\", unit=\" chunks\")\n",
    "\n",
    "for file_idx, json_file in enumerate(json_files, 1):\n",
    "    file_size = json_file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"\\nFile {file_idx}/{len(json_files)}: {json_file.name} ({file_size:.1f} MB)\")\n",
    "\n",
    "    # Load entire file (no streaming — Colab has plenty of RAM)\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    # Filter valid records\n",
    "    records = []\n",
    "    skipped = 0\n",
    "    for rec in raw_data:\n",
    "        rid = rec.get(\"id\")\n",
    "        content = rec.get(\"content\")\n",
    "        if rid is None or not isinstance(content, str) or not content.strip():\n",
    "            skipped += 1\n",
    "            continue\n",
    "        records.append(rec)\n",
    "\n",
    "    total_skipped += skipped\n",
    "    print(f\"  Loaded {len(records)} valid records, skipped {skipped}\")\n",
    "    del raw_data\n",
    "\n",
    "    # Process in upsert-sized batches\n",
    "    for i in range(0, len(records), UPSERT_BATCH_SIZE):\n",
    "        batch = records[i : i + UPSERT_BATCH_SIZE]\n",
    "        print(f\"  Batch {i//UPSERT_BATCH_SIZE + 1}: embedding + upserting {len(batch)} chunks...\")\n",
    "        count = _process_and_upsert(batch, model, qdrant, COLLECTION_NAME, EMBED_BATCH_SIZE)\n",
    "        total_ingested += count\n",
    "        pbar.update(count)\n",
    "\n",
    "    del records\n",
    "    gc.collect()\n",
    "\n",
    "pbar.close()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"Ingestion complete!\")\n",
    "print(f\"  Ingested : {total_ingested} chunks\")\n",
    "print(f\"  Skipped  : {total_skipped} chunks\")\n",
    "print(f\"  Files    : {len(json_files)}\")\n",
    "print(f\"  Time     : {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "print(f\"  Speed    : {total_ingested/elapsed:.1f} chunks/sec\")\n",
    "print(f\"{'=' * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d392e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 8. Verify Ingestion\n",
    "\n",
    "Check that all points were stored correctly in Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c1b99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T12:06:07.803317Z",
     "iopub.status.busy": "2026-02-16T12:06:07.803016Z",
     "iopub.status.idle": "2026-02-16T12:06:08.037958Z",
     "shell.execute_reply": "2026-02-16T12:06:08.037291Z",
     "shell.execute_reply.started": "2026-02-16T12:06:07.803293Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def verify_ingestion(client: QdrantClient, collection: str) -> None:\n",
    "    \"\"\"Print collection stats and a sample point to verify ingestion.\"\"\"\n",
    "    info = client.get_collection(collection)\n",
    "    print(f\"Collection       : {collection}\")\n",
    "    print(f\"Total points     : {info.points_count}\")\n",
    "    print(f\"Vector dimension : {info.config.params.vectors.size}\")\n",
    "    print(f\"Distance metric  : {info.config.params.vectors.distance}\")\n",
    "    print(f\"HNSW m           : {info.config.hnsw_config.m}\")\n",
    "    print(f\"On-disk payload  : {info.config.params.on_disk_payload}\")\n",
    "\n",
    "    # Fetch one sample point to verify structure\n",
    "    sample = client.scroll(\n",
    "        collection_name=collection,\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False,\n",
    "    )\n",
    "    if sample[0]:\n",
    "        point = sample[0][0]\n",
    "        print(f\"\\nSample point ID  : {point.id}\")\n",
    "        print(f\"Sample payload   :\")\n",
    "        for key, val in point.payload.items():\n",
    "            display = str(val)[:80] + \"...\" if len(str(val)) > 80 else str(val)\n",
    "            print(f\"  {key}: {display}\")\n",
    "\n",
    "\n",
    "verify_ingestion(qdrant, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a0011",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 9. Quick Search Test\n",
    "\n",
    "Run a quick similarity search to make sure vectors are working.  \n",
    "This mimics what the deployed RAG service does at query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a82f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T11:43:29.349299Z",
     "iopub.status.busy": "2026-02-16T11:43:29.348919Z",
     "iopub.status.idle": "2026-02-16T11:43:29.582900Z",
     "shell.execute_reply": "2026-02-16T11:43:29.582353Z",
     "shell.execute_reply.started": "2026-02-16T11:43:29.349276Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_search(\n",
    "    model: SentenceTransformer,\n",
    "    client: QdrantClient,\n",
    "    collection: str,\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    ") -> None:\n",
    "    \"\"\"Run a test similarity search and print results.\n",
    "\n",
    "    Uses the BGE query prefix for proper query embedding.\n",
    "    Searches with exact=True (brute-force) like the deployed service.\n",
    "    \"\"\"\n",
    "    from qdrant_client.http.models import SearchParams\n",
    "\n",
    "    query_prefix = \"Represent this sentence for searching relevant passages: \"\n",
    "    prefixed = query_prefix + query\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vector = model.encode(\n",
    "            [prefixed],\n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True,\n",
    "        )[0].tolist()\n",
    "\n",
    "    results = client.search(\n",
    "        collection_name=collection,\n",
    "        query_vector=vector,\n",
    "        search_params=SearchParams(exact=True),\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        with_vectors=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Top {top_k} results:\\n\")\n",
    "    for i, hit in enumerate(results, 1):\n",
    "        p = hit.payload\n",
    "        content_preview = p.get('content', '')[:150].replace('\\n', ' ')\n",
    "        print(f\"  [{i}] Score: {hit.score:.4f}\")\n",
    "        print(f\"      Source: {p.get('source', 'N/A')} | Page: {p.get('page_number', 'N/A')}\")\n",
    "        print(f\"      {content_preview}...\")\n",
    "        print()\n",
    "\n",
    "\n",
    "test_search(model, qdrant, COLLECTION_NAME, \"What is retrieval augmented generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ea0d2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## Done!\n",
    "\n",
    "Your vectors are now stored in Qdrant. The deployed `rag-service` will:\n",
    "1. Receive a query + candidate IDs from the Lucene service\n",
    "2. Embed the query using the same `BAAI/bge-small-en` model\n",
    "3. Search Qdrant with `HasIdCondition` (filtered brute-force)\n",
    "4. Return the top 10 chunks to Claude for answer generation\n",
    "\n",
    "Make sure your deployed Qdrant instance has this data before starting the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626adcc1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b805076",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9482764,
     "sourceId": 14857679,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-18T17:48:23.677820",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}