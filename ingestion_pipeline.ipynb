{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production RAG — Ingestion Pipeline\n",
    "\n",
    "This notebook embeds chunked documents and stores them in Qdrant.  \n",
    "Designed to run on **Google Colab** (free GPU) so ingestion finishes in minutes, not hours.\n",
    "\n",
    "### What this notebook does\n",
    "1. Installs dependencies (torch GPU, sentence-transformers, qdrant-client)\n",
    "2. Uploads the JSON chunk exports from the Lucene service\n",
    "3. Loads `BAAI/bge-small-en` embedding model on GPU\n",
    "4. Creates a Qdrant collection (HNSW disabled, brute-force only)\n",
    "5. Streams each JSON file → embeds in batches → upserts to Qdrant\n",
    "6. Verifies the final point count\n",
    "\n",
    "### Requirements\n",
    "- The 11 JSON files from `lucene-service/chunk-exports/`\n",
    "- A running Qdrant instance (local, Docker, or Qdrant Cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers==3.3.1 qdrant-client==1.12.1 torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "All settings in one place. Update `QDRANT_HOST` / `QDRANT_API_KEY` to point to your Qdrant instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import uuid\nimport gc\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Generator\n\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import (\n    Distance,\n    HnswConfigDiff,\n    OptimizersConfigDiff,\n    PointStruct,\n    VectorParams,\n)\nfrom tqdm.auto import tqdm\n\n\n# ── Qdrant Cloud ────────────────────────────────────────────\nQDRANT_URL = \"https://b210317b-feb7-4514-89c0-44668fffeba0.eu-central-1-0.aws.cloud.qdrant.io:6333\"\nQDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.MC3P9BZdG63yqfXKnG3udz5XAyS-wbqctc52fEcmYGk\"\nCOLLECTION_NAME = \"rag_chunks\"\n\n# ── Embedding ───────────────────────────────────────────────\nEMBEDDING_MODEL = \"BAAI/bge-small-en\"\nEMBEDDING_DIM = 384\nEMBED_BATCH_SIZE = 256             # GPU can handle larger batches\n\n# ── Ingestion ───────────────────────────────────────────────\nUPSERT_BATCH_SIZE = 1000           # Points per Qdrant upsert call\nJSON_DIR = \"./chunk-exports\"       # Upload your JSON files here\n\n# ── Device ──────────────────────────────────────────────────\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device : {DEVICE}\")\nprint(f\"PyTorch: {torch.__version__}\")\nif DEVICE == \"cuda\":\n    print(f\"GPU    : {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Upload JSON Chunk Exports\n",
    "\n",
    "Upload the 11 JSON files exported by the Lucene service.  \n",
    "Two options:\n",
    "- **Option A**: Google Colab file upload (small files)\n",
    "- **Option B**: Mount Google Drive (recommended for ~99 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Option A: Upload directly (works for small files) ──────\n",
    "# Uncomment the lines below to upload via Colab UI:\n",
    "\n",
    "# from google.colab import files\n",
    "# os.makedirs(JSON_DIR, exist_ok=True)\n",
    "# uploaded = files.upload()\n",
    "# for name, data in uploaded.items():\n",
    "#     with open(os.path.join(JSON_DIR, name), 'wb') as f:\n",
    "#         f.write(data)\n",
    "# print(f\"Uploaded {len(uploaded)} file(s)\")\n",
    "\n",
    "\n",
    "# ── Option B: Mount Google Drive (recommended) ─────────────\n",
    "# Upload the JSON files to your Drive first, then:\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# JSON_DIR = \"/content/drive/MyDrive/chunk-exports\"  # adjust path\n",
    "\n",
    "\n",
    "# ── Verify files ───────────────────────────────────────────\n",
    "json_files = sorted(Path(JSON_DIR).glob(\"*.json\"))\n",
    "total_size = sum(f.stat().st_size for f in json_files) / (1024 * 1024)\n",
    "print(f\"Found {len(json_files)} JSON file(s) — {total_size:.1f} MB total\")\n",
    "for f in json_files:\n",
    "    print(f\"  {f.name} ({f.stat().st_size / (1024*1024):.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Embedding Model\n",
    "\n",
    "Loads `BAAI/bge-small-en` (33M params, 384 dimensions).  \n",
    "On Colab GPU this takes ~5 seconds. On CPU it takes ~30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model_name: str, device: str) -> SentenceTransformer:\n",
    "    \"\"\"Load and return the sentence-transformer model.\n",
    "\n",
    "    The model is put in eval mode and moved to the specified device.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {model_name} on {device}...\")\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded. Max sequence length: {model.max_seq_length}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_embedding_model(EMBEDDING_MODEL, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Connect to Qdrant & Create Collection\n",
    "\n",
    "Creates the collection with **HNSW fully disabled** (`m=0`).  \n",
    "This is intentional — the deployed server has only 1 GB RAM.  \n",
    "Search uses `exact=True` (brute-force) over ~1000 Lucene-filtered candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def connect_qdrant(url: str, api_key: str) -> QdrantClient:\n    \"\"\"Connect to Qdrant Cloud.\n\n    Uses the full URL + API key for authentication.\n    \"\"\"\n    client = QdrantClient(url=url, api_key=api_key, timeout=120)\n    collections = [c.name for c in client.get_collections().collections]\n    print(f\"Connected to Qdrant Cloud. Existing collections: {collections}\")\n    return client\n\n\ndef create_collection(\n    client: QdrantClient,\n    name: str,\n    dim: int,\n) -> None:\n    \"\"\"Create the vector collection with HNSW disabled.\n\n    Design choices:\n      - HnswConfigDiff(m=0)       -> no HNSW graph, saves RAM\n      - indexing_threshold=0       -> no automatic index building\n      - on_disk_payload=True       -> payloads stored on disk, not RAM\n      - Distance.COSINE            -> cosine similarity scoring\n    \"\"\"\n    collections = [c.name for c in client.get_collections().collections]\n    if name in collections:\n        info = client.get_collection(name)\n        print(f\"Collection '{name}' already exists with {info.points_count} points.\")\n        return\n\n    client.create_collection(\n        collection_name=name,\n        vectors_config=VectorParams(\n            size=dim,\n            distance=Distance.COSINE,\n        ),\n        hnsw_config=HnswConfigDiff(m=0),\n        optimizers_config=OptimizersConfigDiff(\n            indexing_threshold=0,\n        ),\n        on_disk_payload=True,\n    )\n    print(f\"Created collection '{name}' (HNSW disabled, on-disk payload).\")\n\n\nqdrant = connect_qdrant(QDRANT_URL, QDRANT_API_KEY)\ncreate_collection(qdrant, COLLECTION_NAME, EMBEDDING_DIM)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Helper Functions\n\nCore utilities used by the ingestion loop:  \n- **`to_qdrant_id`** — deterministic UUID conversion (Qdrant needs valid UUIDs)  \n- **`load_json_records`** — loads and validates records from a JSON file  \n- **`build_payload`** — extracts minimal payload fields for storage  \n- **`embed_batch`** — embeds a list of texts using the model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def to_qdrant_id(chunk_id: str) -> str:\n    \"\"\"Convert an arbitrary chunk ID string to a valid Qdrant UUID.\n\n    Qdrant accepts only unsigned integers or valid UUIDs as point IDs.\n    Lucene exports IDs like '5c4a9c97-..._p1_c0_088c5634' which are not\n    valid UUIDs. uuid5 produces a deterministic UUID from any string,\n    so the same chunk_id always maps to the same Qdrant point ID.\n\n    IMPORTANT: This same function is used in the deployed rag-service\n    (qdrant_store.py) so the IDs match at query time.\n    \"\"\"\n    return str(uuid.uuid5(uuid.NAMESPACE_URL, chunk_id))\n\n\ndef load_json_records(path: Path) -> List[Dict[str, Any]]:\n    \"\"\"Load a JSON file and return validated records.\n\n    Filters out records with missing ID or non-string content.\n    On Colab we have plenty of RAM so json.load() is fine.\n    \"\"\"\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    records = []\n    skipped = 0\n    for rec in data:\n        rid = rec.get(\"id\")\n        content = rec.get(\"content\")\n        if rid is None or not isinstance(content, str) or not content.strip():\n            skipped += 1\n            continue\n        records.append(rec)\n\n    print(f\"  Loaded {len(records)} valid records, skipped {skipped}\")\n    return records\n\n\ndef sanitize_text(text: str) -> str:\n    \"\"\"Clean text for the tokenizer.\n\n    Removes NUL bytes and other control characters that can\n    cause the Rust tokenizer to reject the input.\n    \"\"\"\n    # Remove NUL bytes\n    text = text.replace(\"\\x00\", \"\")\n    # Replace other control chars (keep newline, tab, carriage return)\n    cleaned = []\n    for ch in text:\n        if ord(ch) < 32 and ch not in \"\\n\\r\\t\":\n            cleaned.append(\" \")\n        else:\n            cleaned.append(ch)\n    return \"\".join(cleaned).strip()\n\n\ndef build_payload(record: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Extract minimal payload to store alongside the vector.\n\n    Only fields needed for response construction are kept.\n    The original chunk_id is stored so the API can return it.\n    \"\"\"\n    metadata = record.get(\"metadata\", {})\n    return {\n        \"chunk_id\": record.get(\"id\", \"\"),\n        \"content\": record.get(\"content\", \"\"),\n        \"source\": metadata.get(\"source\", \"\"),\n        \"title\": metadata.get(\"title\", \"\"),\n        \"page_number\": metadata.get(\"page_number\"),\n        \"chunk_index\": metadata.get(\"chunk_index\"),\n        \"document_id\": record.get(\"document_id\", \"\"),\n    }\n\n\ndef embed_batch(\n    model: SentenceTransformer,\n    texts: List[str],\n    batch_size: int = 256,\n) -> List[List[float]]:\n    \"\"\"Embed a list of texts and return as list of float vectors.\n\n    Uses torch.no_grad() to save memory. Normalizes embeddings\n    for cosine similarity. Cleans up GPU memory after encoding.\n    \"\"\"\n    with torch.no_grad():\n        embeddings = model.encode(\n            texts,\n            batch_size=batch_size,\n            show_progress_bar=False,\n            normalize_embeddings=True,\n            convert_to_numpy=True,\n        )\n    result = embeddings.tolist()\n    del embeddings\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    return result\n\n\nprint(\"Helper functions loaded.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Run Ingestion Pipeline\n\nThis is the main loop:  \n1. Loads each JSON file with `json.load()`  \n2. Sanitizes text content for the tokenizer  \n3. Embeds in batches of 256 on GPU  \n4. Upserts to Qdrant Cloud in batches of 1000  \n5. Reports progress with a live progress bar  \n\nOn a **Colab T4 GPU**, expect ~99 MB of chunks to finish in **2-5 minutes**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _process_and_upsert(\n    records: List[Dict[str, Any]],\n    model: SentenceTransformer,\n    client: QdrantClient,\n    collection: str,\n    embed_batch_size: int,\n) -> int:\n    \"\"\"Embed a batch of records and upsert to Qdrant.\"\"\"\n    # Sanitize texts for the tokenizer\n    texts = [sanitize_text(str(r[\"content\"])) for r in records]\n    raw_ids = [r[\"id\"] for r in records]\n    ids = [to_qdrant_id(rid) for rid in raw_ids]\n    payloads = [build_payload(r) for r in records]\n\n    # Embed in sub-batches\n    all_vectors: List[List[float]] = []\n    for i in range(0, len(texts), embed_batch_size):\n        sub = texts[i : i + embed_batch_size]\n        vecs = embed_batch(model, sub, batch_size=embed_batch_size)\n        all_vectors.extend(vecs)\n        del vecs\n\n    # Build points and upsert\n    points = [\n        PointStruct(id=ids[j], vector=all_vectors[j], payload=payloads[j])\n        for j in range(len(ids))\n    ]\n    client.upsert(collection_name=collection, points=points, wait=True)\n\n    del all_vectors, points\n    gc.collect()\n    return len(ids)\n\n\n# ── Main ingestion loop ────────────────────────────────────\ntotal_ingested = 0\ntotal_skipped = 0\nstart_time = time.time()\npbar = tqdm(desc=\"Ingesting\", unit=\" chunks\")\n\nfor file_idx, json_file in enumerate(json_files, 1):\n    file_size = json_file.stat().st_size / (1024 * 1024)\n    print(f\"\\nFile {file_idx}/{len(json_files)}: {json_file.name} ({file_size:.1f} MB)\")\n\n    # Load entire file (no streaming — Colab has plenty of RAM)\n    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n        raw_data = json.load(f)\n\n    # Filter valid records\n    records = []\n    skipped = 0\n    for rec in raw_data:\n        rid = rec.get(\"id\")\n        content = rec.get(\"content\")\n        if rid is None or not isinstance(content, str) or not content.strip():\n            skipped += 1\n            continue\n        records.append(rec)\n\n    total_skipped += skipped\n    print(f\"  Loaded {len(records)} valid records, skipped {skipped}\")\n    del raw_data\n\n    # Process in upsert-sized batches\n    for i in range(0, len(records), UPSERT_BATCH_SIZE):\n        batch = records[i : i + UPSERT_BATCH_SIZE]\n        print(f\"  Batch {i//UPSERT_BATCH_SIZE + 1}: embedding + upserting {len(batch)} chunks...\")\n        count = _process_and_upsert(batch, model, qdrant, COLLECTION_NAME, EMBED_BATCH_SIZE)\n        total_ingested += count\n        pbar.update(count)\n\n    del records\n    gc.collect()\n\npbar.close()\nelapsed = time.time() - start_time\n\nprint(f\"\\n{'=' * 50}\")\nprint(f\"Ingestion complete!\")\nprint(f\"  Ingested : {total_ingested} chunks\")\nprint(f\"  Skipped  : {total_skipped} chunks\")\nprint(f\"  Files    : {len(json_files)}\")\nprint(f\"  Time     : {elapsed:.1f}s ({elapsed/60:.1f} min)\")\nprint(f\"  Speed    : {total_ingested/elapsed:.1f} chunks/sec\")\nprint(f\"{'=' * 50}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Verify Ingestion\n",
    "\n",
    "Check that all points were stored correctly in Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_ingestion(client: QdrantClient, collection: str) -> None:\n",
    "    \"\"\"Print collection stats and a sample point to verify ingestion.\"\"\"\n",
    "    info = client.get_collection(collection)\n",
    "    print(f\"Collection       : {collection}\")\n",
    "    print(f\"Total points     : {info.points_count}\")\n",
    "    print(f\"Vector dimension : {info.config.params.vectors.size}\")\n",
    "    print(f\"Distance metric  : {info.config.params.vectors.distance}\")\n",
    "    print(f\"HNSW m           : {info.config.hnsw_config.m}\")\n",
    "    print(f\"On-disk payload  : {info.config.params.on_disk_payload}\")\n",
    "\n",
    "    # Fetch one sample point to verify structure\n",
    "    sample = client.scroll(\n",
    "        collection_name=collection,\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False,\n",
    "    )\n",
    "    if sample[0]:\n",
    "        point = sample[0][0]\n",
    "        print(f\"\\nSample point ID  : {point.id}\")\n",
    "        print(f\"Sample payload   :\")\n",
    "        for key, val in point.payload.items():\n",
    "            display = str(val)[:80] + \"...\" if len(str(val)) > 80 else str(val)\n",
    "            print(f\"  {key}: {display}\")\n",
    "\n",
    "\n",
    "verify_ingestion(qdrant, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Quick Search Test\n",
    "\n",
    "Run a quick similarity search to make sure vectors are working.  \n",
    "This mimics what the deployed RAG service does at query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_search(\n",
    "    model: SentenceTransformer,\n",
    "    client: QdrantClient,\n",
    "    collection: str,\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    ") -> None:\n",
    "    \"\"\"Run a test similarity search and print results.\n",
    "\n",
    "    Uses the BGE query prefix for proper query embedding.\n",
    "    Searches with exact=True (brute-force) like the deployed service.\n",
    "    \"\"\"\n",
    "    from qdrant_client.http.models import SearchParams\n",
    "\n",
    "    query_prefix = \"Represent this sentence for searching relevant passages: \"\n",
    "    prefixed = query_prefix + query\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vector = model.encode(\n",
    "            [prefixed],\n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True,\n",
    "        )[0].tolist()\n",
    "\n",
    "    results = client.search(\n",
    "        collection_name=collection,\n",
    "        query_vector=vector,\n",
    "        search_params=SearchParams(exact=True),\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        with_vectors=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Top {top_k} results:\\n\")\n",
    "    for i, hit in enumerate(results, 1):\n",
    "        p = hit.payload\n",
    "        content_preview = p.get('content', '')[:150].replace('\\n', ' ')\n",
    "        print(f\"  [{i}] Score: {hit.score:.4f}\")\n",
    "        print(f\"      Source: {p.get('source', 'N/A')} | Page: {p.get('page_number', 'N/A')}\")\n",
    "        print(f\"      {content_preview}...\")\n",
    "        print()\n",
    "\n",
    "\n",
    "test_search(model, qdrant, COLLECTION_NAME, \"What is retrieval augmented generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Done!\n",
    "\n",
    "Your vectors are now stored in Qdrant. The deployed `rag-service` will:\n",
    "1. Receive a query + candidate IDs from the Lucene service\n",
    "2. Embed the query using the same `BAAI/bge-small-en` model\n",
    "3. Search Qdrant with `HasIdCondition` (filtered brute-force)\n",
    "4. Return the top 10 chunks to Claude for answer generation\n",
    "\n",
    "Make sure your deployed Qdrant instance has this data before starting the server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}